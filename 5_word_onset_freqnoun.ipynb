{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import mne\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from model import *\n",
    "from dataset import *\n",
    "from utils import *\n",
    "from train import *\n",
    "\n",
    "# Check if GPU is available\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.cuda.empty_cache()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_story_list = dict(\n",
    "    Moth1=[\"souls\", \"avatar\", \"legacy\", \"odetostepfather\"],\n",
    "    Moth2=[\"howtodraw\", \"myfirstdaywiththeyankees\", \"naked\", \"life\"],\n",
    "    Moth3=[\"tildeath\", \"fromboyhoodtofatherhood\", \"sloth\", \"exorcism\"],\n",
    "    Moth4=[\"adollshouse\", \"inamoment\", \"theclosetthatateeverything\", \"adventuresinsayingyes\", \"haveyoumethimyet\"],\n",
    "    Moth5=[\"thatthingonmyarm\", \"eyespy\", \"itsabox\", \"hangtime\"],\n",
    ")\n",
    "train_stories = [story for session in train_story_list.keys() for story in train_story_list[session]]\n",
    "val_stories = [[\"swimmingwithastronauts1\", \"swimmingwithastronauts2\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alternateithicatom1',\n",
       " 'souls',\n",
       " 'wheretheressmoke1',\n",
       " 'avatar',\n",
       " 'legacy',\n",
       " 'odetostepfather',\n",
       " 'undertheinfluence1',\n",
       " 'howtodraw',\n",
       " 'myfirstdaywiththeyankees',\n",
       " 'naked',\n",
       " 'life',\n",
       " 'stagefright1',\n",
       " 'tildeath',\n",
       " 'fromboyhoodtofatherhood',\n",
       " 'sloth',\n",
       " 'exorcism',\n",
       " 'buck1',\n",
       " 'adollshouse',\n",
       " 'inamoment',\n",
       " 'theclosetthatateeverything',\n",
       " 'adventuresinsayingyes',\n",
       " 'haveyoumethimyet',\n",
       " 'swimmingwithastronauts1',\n",
       " 'thatthingonmyarm',\n",
       " 'eyespy',\n",
       " 'itsabox',\n",
       " 'hangtime']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let test stories be all 27 stories\n",
    "with open(\"data/story_to_uniquestory.pkl\", \"rb\") as f:\n",
    "    story_to_uniquestory = pickle.load(f)\n",
    "test_stories = list(story_to_uniquestory.keys())\n",
    "test_stories = [s for s in test_stories if s != \"stimuli_auditory_localizer\" and not s[-1] in [\"2\", \"3\", \"4\", \"5\"]]\n",
    "test_stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "name = \"WdPnFq-seg8-flexconv4-A\"\n",
    "which = \"val-loss-min\"\n",
    "\n",
    "with open(f\"config/{name}.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "MEG_SUBJECT = config[\"MEG_SUBJECT\"]\n",
    "FMRI_SUBJECT = config[\"FMRI_SUBJECT\"]\n",
    "use_segment = config[\"use_segment\"]\n",
    "spacing = config[\"spacing\"]\n",
    "meg_loss_weight = config[\"meg_loss_weight\"]\n",
    "fmri_loss_weight = config[\"fmri_loss_weight\"]\n",
    "softmax_T = config[\"softmax_T\"]\n",
    "dataset_params = config[\"dataset\"]\n",
    "model_params = config[\"model\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_word: True, use_phoneme: True, use_freq: True, use_meg: False, use_mri: False\n",
      "embed_dim:  852\n",
      "use_word: True, use_phoneme: True, use_freq: True, use_meg: False, use_mri: False\n",
      "embed_dim:  852\n",
      "Loading story swimmingwithastronauts1...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading story swimmingwithastronauts1!\n",
      "Loading story swimmingwithastronauts2...\n",
      "Finished loading story swimmingwithastronauts2!\n",
      "Preloaded all stories!\n",
      "use_word: True, use_phoneme: True, use_freq: True, use_meg: False, use_mri: False\n",
      "embed_dim:  852\n",
      "Loading story alternateithicatom1...\n",
      "Finished loading story alternateithicatom1!\n",
      "Loading story souls...\n",
      "Finished loading story souls!\n",
      "Loading story wheretheressmoke1...\n",
      "Finished loading story wheretheressmoke1!\n",
      "Loading story avatar...\n",
      "Finished loading story avatar!\n",
      "Loading story legacy...\n",
      "Finished loading story legacy!\n",
      "Loading story odetostepfather...\n",
      "Finished loading story odetostepfather!\n",
      "Loading story undertheinfluence1...\n",
      "Finished loading story undertheinfluence1!\n",
      "Loading story howtodraw...\n",
      "Finished loading story howtodraw!\n",
      "Loading story myfirstdaywiththeyankees...\n",
      "Finished loading story myfirstdaywiththeyankees!\n",
      "Loading story naked...\n",
      "Finished loading story naked!\n",
      "Loading story life...\n",
      "Finished loading story life!\n",
      "Loading story stagefright1...\n",
      "Finished loading story stagefright1!\n",
      "Loading story tildeath...\n",
      "Finished loading story tildeath!\n",
      "Loading story fromboyhoodtofatherhood...\n",
      "Finished loading story fromboyhoodtofatherhood!\n",
      "Loading story sloth...\n",
      "Finished loading story sloth!\n",
      "Loading story exorcism...\n",
      "Finished loading story exorcism!\n",
      "Loading story buck1...\n",
      "Finished loading story buck1!\n",
      "Loading story adollshouse...\n",
      "Finished loading story adollshouse!\n",
      "Loading story inamoment...\n",
      "Finished loading story inamoment!\n",
      "Loading story theclosetthatateeverything...\n",
      "Finished loading story theclosetthatateeverything!\n",
      "Loading story adventuresinsayingyes...\n",
      "Finished loading story adventuresinsayingyes!\n",
      "Loading story haveyoumethimyet...\n",
      "Finished loading story haveyoumethimyet!\n",
      "Loading story swimmingwithastronauts1...\n",
      "Finished loading story swimmingwithastronauts1!\n",
      "Loading story thatthingonmyarm...\n",
      "Finished loading story thatthingonmyarm!\n",
      "Loading story eyespy...\n",
      "Finished loading story eyespy!\n",
      "Loading story itsabox...\n",
      "Finished loading story itsabox!\n",
      "Loading story hangtime...\n",
      "Finished loading story hangtime!\n",
      "Preloaded all stories!\n"
     ]
    }
   ],
   "source": [
    "train_dataset = StoryDataset(\n",
    "    MEG_SUBJECT,\n",
    "    FMRI_SUBJECT,\n",
    "    train_stories[0:1],\n",
    "    name=name,\n",
    "    spacing=spacing,\n",
    "    preload=False,\n",
    "    **dataset_params,\n",
    ")  # evalulate don't care this\n",
    "val_dataset = StoryDataset(\n",
    "    MEG_SUBJECT,\n",
    "    FMRI_SUBJECT,\n",
    "    val_stories,\n",
    "    name=name,\n",
    "    spacing=spacing,\n",
    "    pca_meg=train_dataset.pca_meg,\n",
    "    pca_mri=train_dataset.pca_mri,\n",
    "    **dataset_params,\n",
    ")\n",
    "test_dataset = StoryDataset(\n",
    "    MEG_SUBJECT,\n",
    "    FMRI_SUBJECT,\n",
    "    test_stories,\n",
    "    name=name,\n",
    "    spacing=spacing,\n",
    "    pca_meg=train_dataset.pca_meg,\n",
    "    pca_mri=train_dataset.pca_mri,\n",
    "    MEG_DIR=\"moth_meg\",\n",
    "    **dataset_params,\n",
    ")\n",
    "embed_dim = test_dataset.embed_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lead Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading forward solution from /home/yishuli/MEG-fMRI/data/A-oct6-fwd.fif...\n",
      "    Reading a source space...\n",
      "    Computing patch statistics...\n",
      "    Patch information added...\n",
      "    [done]\n",
      "    Reading a source space...\n",
      "    Computing patch statistics...\n",
      "    Patch information added...\n",
      "    [done]\n",
      "    2 source spaces read\n",
      "    Desired named matrix (kind = 3523 (FIFF_MNE_FORWARD_SOLUTION_GRAD)) not available\n",
      "    Read MEG forward solution (8196 sources, 306 channels, free orientations)\n",
      "    Source spaces transformed to the forward solution coordinate frame\n",
      "    Average patch normals will be employed in the rotation to the local surface coordinates....\n",
      "    Converting to surface-based source orientations...\n",
      "    [done]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(306, 8196)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load forward solution to get the lead field\n",
    "fname_fwd = f\"data/{MEG_SUBJECT}-{spacing}-fwd.fif\"\n",
    "fwd = mne.read_forward_solution(fname_fwd)\n",
    "fwd_fixed = mne.convert_forward_solution(fwd, surf_ori=True, force_fixed=True, use_cps=True)  # let's do fixed orientation\n",
    "lead_field = fwd_fixed[\"sol\"][\"data\"]\n",
    "lead_field = torch.from_numpy(lead_field)\n",
    "n_channels, n_neurons = lead_field.shape\n",
    "n_channels, n_neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68, 1.0055146501442056, 0.07546357089945342, 0.9299632706765684)\n"
     ]
    }
   ],
   "source": [
    "model = TransformerSourceModel(\n",
    "    embed_dim=embed_dim,\n",
    "    lead_field=lead_field,\n",
    "    **model_params,\n",
    ").to(device)\n",
    "ckpt_path = f\"trained_models/{name}_{which}.pth\"\n",
    "info = load_checkpoint(ckpt_path, model, None, None, device=device)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_meg_loss: 0.07466866387896606, val_fmri_loss: 0.9308459862652395\n"
     ]
    }
   ],
   "source": [
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "with torch.no_grad():\n",
    "    validate(\n",
    "        model,\n",
    "        val_dataloader,\n",
    "        device,\n",
    "        subject=MEG_SUBJECT,\n",
    "        meg_loss_weight=meg_loss_weight,\n",
    "        fmri_loss_weight=fmri_loss_weight,\n",
    "        softmax_T=softmax_T,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "neurons_dict, neurons_power_dict = {}, {}\n",
    "for i in range(len(test_dataset)):\n",
    "    # load data\n",
    "    embeds, meg, fmri = test_dataset[i]\n",
    "    embeds = embeds.to(device)\n",
    "    # foward pass\n",
    "    with torch.no_grad():\n",
    "        neurons, _, _ = model(embeds.unsqueeze(0))\n",
    "        neurons = neurons.squeeze(0).detach().cpu()\n",
    "        # calculate power\n",
    "        neurons_power = hilbert_torch(neurons, dim=0)\n",
    "        neurons_power = neurons_power.abs()\n",
    "        neurons_power.pow_(2)\n",
    "        # demean\n",
    "        neurons = (neurons.abs() - neurons.abs().mean(dim=0, keepdims=True)).numpy()\n",
    "        neurons_power = (neurons_power - neurons_power.mean(dim=0, keepdims=True)).numpy()\n",
    "    neurons_dict[test_stories[i]] = neurons\n",
    "    neurons_power_dict[test_stories[i]] = neurons_power\n",
    "    # save memory\n",
    "    del embeds\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Onsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load grid transcript\n",
    "with open(\"data/moth_word_surprisal_context20.pkl\", \"rb\") as fp:\n",
    "    story_word_surprisal_dict = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_dict = {}\n",
    "for key in story_word_surprisal_dict.keys():\n",
    "    if key == \"stimuli_auditory_localizer\":\n",
    "        continue\n",
    "    for t in story_word_surprisal_dict[key]:\n",
    "        word = t[2].lower().strip()\n",
    "        if t[-1] == \"NOUN\":\n",
    "            if word not in all_words_dict:\n",
    "                all_words_dict[word] = 0\n",
    "            else:\n",
    "                all_words_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 1758,\n",
       " 'uh': 181,\n",
       " 'um': 149,\n",
       " 'time': 125,\n",
       " 'day': 94,\n",
       " 'thing': 93,\n",
       " 'kind': 90,\n",
       " 'people': 82,\n",
       " 'home': 78,\n",
       " 'something': 76,\n",
       " 'things': 74,\n",
       " 'years': 72,\n",
       " 'way': 69,\n",
       " 'life': 66,\n",
       " 'right': 66,\n",
       " 'one': 51,\n",
       " 'house': 50,\n",
       " 'job': 46,\n",
       " 'school': 46,\n",
       " 'guy': 45,\n",
       " 'family': 45,\n",
       " 'room': 45,\n",
       " 'oh': 45,\n",
       " 'man': 44,\n",
       " 'moment': 43,\n",
       " 'mother': 43,\n",
       " 'water': 43,\n",
       " 'everything': 41,\n",
       " 'mom': 39,\n",
       " 'phone': 38,\n",
       " 'cause': 38,\n",
       " 'stuff': 38,\n",
       " 'lot': 36,\n",
       " 'car': 36,\n",
       " 'friends': 35,\n",
       " 'night': 35,\n",
       " 'anything': 35,\n",
       " 'year': 35,\n",
       " 'door': 34,\n",
       " 'twenty': 33,\n",
       " 'whole': 32,\n",
       " 'sort': 32,\n",
       " 'dad': 31,\n",
       " 'everybody': 29,\n",
       " 'girl': 29,\n",
       " 'head': 29,\n",
       " 'look': 29,\n",
       " 'president': 29,\n",
       " 'hand': 28,\n",
       " 'yeah': 27,\n",
       " 'parents': 27,\n",
       " 'york': 26,\n",
       " 'work': 26,\n",
       " 'boy': 26,\n",
       " 'birthday': 26,\n",
       " 'okay': 26,\n",
       " 'thought': 25,\n",
       " 'story': 25,\n",
       " 'someone': 25,\n",
       " 'stage': 25,\n",
       " 'god': 24,\n",
       " 'thirty': 24,\n",
       " 'father': 24,\n",
       " 'bat': 24,\n",
       " 'couple': 23,\n",
       " 'children': 23,\n",
       " 'book': 23,\n",
       " 'front': 23,\n",
       " 'call': 23,\n",
       " 'point': 23,\n",
       " 'yes': 22,\n",
       " 'morning': 22,\n",
       " 'somebody': 22,\n",
       " 'alright': 22,\n",
       " 'weeks': 22,\n",
       " 'box': 22,\n",
       " 'office': 22,\n",
       " 'side': 21,\n",
       " 'week': 21,\n",
       " 'place': 21,\n",
       " 'hope': 21,\n",
       " 'end': 21,\n",
       " 'world': 21,\n",
       " 'bit': 21,\n",
       " 'eye': 21,\n",
       " 'card': 21,\n",
       " 'name': 20,\n",
       " 'wife': 20,\n",
       " 'person': 20,\n",
       " 'friend': 20,\n",
       " 'start': 20,\n",
       " 'back': 20,\n",
       " 'girls': 20,\n",
       " 'experience': 20,\n",
       " 'hospital': 20,\n",
       " 'camp': 19,\n",
       " 'woman': 19,\n",
       " 'bed': 19,\n",
       " 'times': 19,\n",
       " 'everyone': 19,\n",
       " 'kid': 18,\n",
       " 'help': 18,\n",
       " 's': 18,\n",
       " 'child': 18,\n",
       " 'reason': 18,\n",
       " 'days': 18,\n",
       " 'eyes': 18,\n",
       " 'thank': 18,\n",
       " 'street': 18,\n",
       " 'hands': 18,\n",
       " 'michael': 18,\n",
       " 'hour': 17,\n",
       " 'college': 17,\n",
       " 'nothing': 17,\n",
       " 'ram': 17,\n",
       " 'boat': 17,\n",
       " 'months': 16,\n",
       " 'hair': 16,\n",
       " 'kids': 16,\n",
       " 'get': 16,\n",
       " 'husband': 16,\n",
       " 'money': 16,\n",
       " 'remember': 16,\n",
       " 'cards': 16,\n",
       " 'minutes': 16,\n",
       " 'bus': 16,\n",
       " 'hours': 15,\n",
       " 'part': 15,\n",
       " 'thinking': 15,\n",
       " 'cool': 15,\n",
       " 'daughter': 15,\n",
       " 'football': 15,\n",
       " 'living': 15,\n",
       " 'bum': 15,\n",
       " 'alternate': 14,\n",
       " 'problem': 14,\n",
       " 'arm': 14,\n",
       " 'course': 14,\n",
       " 'idea': 14,\n",
       " 'art': 14,\n",
       " 'couch': 14,\n",
       " 'heard': 13,\n",
       " 'fact': 13,\n",
       " 'business': 13,\n",
       " 'stories': 13,\n",
       " 'mind': 13,\n",
       " 'half': 13,\n",
       " 'group': 13,\n",
       " 'song': 13,\n",
       " 'drop': 13,\n",
       " 'van': 12,\n",
       " 'forty': 12,\n",
       " 'love': 12,\n",
       " 'cigarettes': 12,\n",
       " 'lights': 12,\n",
       " 'blood': 12,\n",
       " 'kinda': 12,\n",
       " 'case': 12,\n",
       " 'son': 12,\n",
       " 'model': 12,\n",
       " 'stadium': 12,\n",
       " 'show': 12,\n",
       " 'sam': 12,\n",
       " 'obama': 12,\n",
       " 'teacher': 11,\n",
       " 'rest': 11,\n",
       " 'kay': 11,\n",
       " 'women': 11,\n",
       " 'party': 11,\n",
       " 'ok': 11,\n",
       " 'hey': 11,\n",
       " 'walk': 11,\n",
       " 'boyfriend': 11,\n",
       " 'baby': 11,\n",
       " 'body': 11,\n",
       " 'voice': 11,\n",
       " 'guys': 11,\n",
       " 'age': 11,\n",
       " 'eighty': 11,\n",
       " 'ten': 11,\n",
       " 'questions': 11,\n",
       " 'left': 11,\n",
       " 'tom': 10,\n",
       " 'road': 10,\n",
       " 'thousand': 10,\n",
       " 'ithaca': 10,\n",
       " 'town': 10,\n",
       " 'mean': 10,\n",
       " 'store': 10,\n",
       " 'number': 10,\n",
       " 'face': 10,\n",
       " 'jt': 10,\n",
       " 'tell': 10,\n",
       " 'g': 10,\n",
       " 'cat': 10,\n",
       " 'guess': 10,\n",
       " 'ass': 10,\n",
       " 'gay': 10,\n",
       " 'men': 10,\n",
       " 'joke': 10,\n",
       " 'fake': 10,\n",
       " 'feet': 10,\n",
       " 'mike': 10,\n",
       " 'depression': 10,\n",
       " 'christmas': 10,\n",
       " 'aunt': 10,\n",
       " 'chi': 9,\n",
       " 'question': 9,\n",
       " 'wow': 9,\n",
       " 'lives': 9,\n",
       " 'letter': 9,\n",
       " 'neighborhood': 9,\n",
       " 'ground': 9,\n",
       " 'grade': 9,\n",
       " 'class': 9,\n",
       " 'sign': 9,\n",
       " 'test': 9,\n",
       " 'die': 9,\n",
       " 'sense': 9,\n",
       " 'yankee': 9,\n",
       " 'stretcher': 9,\n",
       " 'closet': 9,\n",
       " 'shit': 9,\n",
       " 'pictures': 9,\n",
       " 'wait': 9,\n",
       " 'ndepp': 9,\n",
       " 'top': 8,\n",
       " 'drove': 8,\n",
       " 'brother': 8,\n",
       " 'clothes': 8,\n",
       " 'state': 8,\n",
       " 'line': 8,\n",
       " 'pack': 8,\n",
       " 'piece': 8,\n",
       " 'light': 8,\n",
       " 'figure': 8,\n",
       " 'cigarette': 8,\n",
       " 'think': 8,\n",
       " 'dollar': 8,\n",
       " 'blue': 8,\n",
       " 'sit': 8,\n",
       " 'parent': 8,\n",
       " 'step': 8,\n",
       " 'chance': 8,\n",
       " 'sister': 8,\n",
       " 'boys': 8,\n",
       " 'buck': 8,\n",
       " 'let': 8,\n",
       " 'flight': 8,\n",
       " 'don': 8,\n",
       " 'dollars': 8,\n",
       " 'wine': 8,\n",
       " 'sin': 8,\n",
       " 'prison': 8,\n",
       " 'scar': 8,\n",
       " 'helicopter': 8,\n",
       " 'tai': 7,\n",
       " 'deal': 7,\n",
       " 'miles': 7,\n",
       " 'anybody': 7,\n",
       " 'heart': 7,\n",
       " 'care': 7,\n",
       " 'bag': 7,\n",
       " 'church': 7,\n",
       " 'conversation': 7,\n",
       " 'pull': 7,\n",
       " 'ch': 7,\n",
       " 'fire': 7,\n",
       " 'fight': 7,\n",
       " 'come': 7,\n",
       " 'yard': 7,\n",
       " 'matter': 7,\n",
       " 'towards': 7,\n",
       " 'desk': 7,\n",
       " 'brain': 7,\n",
       " 'survival': 7,\n",
       " 'm': 7,\n",
       " 'texas': 7,\n",
       " 'stop': 7,\n",
       " 'word': 7,\n",
       " 'ninety': 7,\n",
       " 'shoulder': 7,\n",
       " 'power': 7,\n",
       " 'manager': 7,\n",
       " 'hi': 7,\n",
       " 'looks': 7,\n",
       " 'tv': 7,\n",
       " 'club': 7,\n",
       " 'pair': 7,\n",
       " 'roll': 7,\n",
       " 'floor': 7,\n",
       " 'brothers': 7,\n",
       " 'air': 7,\n",
       " 'anyone': 7,\n",
       " 'reunion': 7,\n",
       " 'dinner': 7,\n",
       " 'states': 7,\n",
       " 'set': 7,\n",
       " 'gate': 7,\n",
       " 'release': 7,\n",
       " 'aunts': 7,\n",
       " 'beach': 7,\n",
       " 'universe': 6,\n",
       " 'event': 6,\n",
       " 'coin': 6,\n",
       " 'career': 6,\n",
       " 'country': 6,\n",
       " 'answer': 6,\n",
       " 'summer': 6,\n",
       " 'fifteen': 6,\n",
       " 'visit': 6,\n",
       " 'paper': 6,\n",
       " 'make': 6,\n",
       " 'words': 6,\n",
       " 'team': 6,\n",
       " 'meeting': 6,\n",
       " 'food': 6,\n",
       " 'foot': 6,\n",
       " 'sound': 6,\n",
       " 'beer': 6,\n",
       " 'fuck': 6,\n",
       " 'apartment': 6,\n",
       " 'interview': 6,\n",
       " 'movie': 6,\n",
       " 'mine': 6,\n",
       " 'books': 6,\n",
       " 'f': 6,\n",
       " 'letters': 6,\n",
       " 'meant': 6,\n",
       " 'arms': 6,\n",
       " 'memory': 6,\n",
       " 'table': 6,\n",
       " 'influence': 6,\n",
       " 'tonight': 6,\n",
       " 'today': 6,\n",
       " 'see': 6,\n",
       " 'jacket': 6,\n",
       " 'ra': 6,\n",
       " 'bunch': 6,\n",
       " 'process': 6,\n",
       " 'tree': 6,\n",
       " 'thanks': 6,\n",
       " 'pieces': 6,\n",
       " 'weight': 6,\n",
       " 'starts': 6,\n",
       " 'yankees': 6,\n",
       " 'player': 6,\n",
       " 'seat': 6,\n",
       " 'standing': 6,\n",
       " 'while': 6,\n",
       " 'dream': 6,\n",
       " 'chloe': 6,\n",
       " 'music': 6,\n",
       " 'video': 6,\n",
       " 'somehow': 6,\n",
       " 'downstairs': 6,\n",
       " 'twelve': 6,\n",
       " 'space': 6,\n",
       " 'minute': 6,\n",
       " 'glass': 6,\n",
       " 'camera': 6,\n",
       " 'please': 6,\n",
       " 'headphones': 6,\n",
       " 'events': 5,\n",
       " 'knew': 5,\n",
       " 'campus': 5,\n",
       " 'lunch': 5,\n",
       " 'lesson': 5,\n",
       " 'cousins': 5,\n",
       " 'research': 5,\n",
       " 'drinking': 5,\n",
       " 'plan': 5,\n",
       " 'ways': 5,\n",
       " 'rock': 5,\n",
       " 'suit': 5,\n",
       " 'company': 5,\n",
       " 'director': 5,\n",
       " 'information': 5,\n",
       " 'community': 5,\n",
       " 'sorry': 5,\n",
       " 'shoes': 5,\n",
       " 'beautiful': 5,\n",
       " 'ride': 5,\n",
       " 'lots': 5,\n",
       " 'fifty': 5,\n",
       " 'pretty': 5,\n",
       " 'future': 5,\n",
       " 'leroy': 5,\n",
       " 'pain': 5,\n",
       " 'problems': 5,\n",
       " 'writer': 5,\n",
       " 'mail': 5,\n",
       " 'blah': 5,\n",
       " 'tests': 5,\n",
       " 'ah': 5,\n",
       " 'instructor': 5,\n",
       " 't': 5,\n",
       " 'game': 5,\n",
       " 'trouble': 5,\n",
       " 'stay': 5,\n",
       " 'pat': 5,\n",
       " 'watch': 5,\n",
       " 'nobody': 5,\n",
       " 'board': 5,\n",
       " 'fine': 5,\n",
       " 'control': 5,\n",
       " 'wall': 5,\n",
       " 'clubhouse': 5,\n",
       " 'mouth': 5,\n",
       " 'shirt': 5,\n",
       " 'assistant': 5,\n",
       " 'feeling': 5,\n",
       " 'bra': 5,\n",
       " 'order': 5,\n",
       " 'silver': 5,\n",
       " 'pregnancy': 5,\n",
       " 'picture': 5,\n",
       " 'practice': 5,\n",
       " 'emergency': 5,\n",
       " 'tea': 5,\n",
       " 'hip': 5,\n",
       " 'glastonbury': 5,\n",
       " 'princeton': 5,\n",
       " 'pool': 5,\n",
       " 'spirits': 5,\n",
       " 'numbers': 5,\n",
       " 'epilepsy': 5,\n",
       " 'adoption': 5,\n",
       " 'chief': 5,\n",
       " 'oval': 5,\n",
       " 'astronaut': 5,\n",
       " 'swim': 5,\n",
       " 'swimmers': 5,\n",
       " 'kennedy': 5,\n",
       " 'universes': 4,\n",
       " 'results': 4,\n",
       " 'heads': 4,\n",
       " 'volkswagen': 4,\n",
       " 'quit': 4,\n",
       " 'move': 4,\n",
       " 'path': 4,\n",
       " 'pick': 4,\n",
       " 'camps': 4,\n",
       " 'watching': 4,\n",
       " 'joy': 4,\n",
       " 'anymore': 4,\n",
       " 'village': 4,\n",
       " 'brown': 4,\n",
       " 'rice': 4,\n",
       " 'fellow': 4,\n",
       " 'bridge': 4,\n",
       " 'sun': 4,\n",
       " 'relationship': 4,\n",
       " 'university': 4,\n",
       " 'image': 4,\n",
       " 'jewelry': 4,\n",
       " 'target': 4,\n",
       " 'doors': 4,\n",
       " 'service': 4,\n",
       " 'string': 4,\n",
       " 'crazy': 4,\n",
       " 'need': 4,\n",
       " 'telling': 4,\n",
       " 'points': 4,\n",
       " 'anger': 4,\n",
       " 'vest': 4,\n",
       " 'pulls': 4,\n",
       " 'second': 4,\n",
       " 'license': 4,\n",
       " 'keys': 4,\n",
       " 'tells': 4,\n",
       " 'trees': 4,\n",
       " 'sarah': 4,\n",
       " 'spread': 4,\n",
       " 'corner': 4,\n",
       " 'reading': 4,\n",
       " 'blonde': 4,\n",
       " 'west': 4,\n",
       " 'death': 4,\n",
       " 'fan': 4,\n",
       " 'month': 4,\n",
       " 'kindergarten': 4,\n",
       " 'similarities': 4,\n",
       " 'socks': 4,\n",
       " 'doctor': 4,\n",
       " 'seventy': 4,\n",
       " 'weird': 4,\n",
       " 'audience': 4,\n",
       " 'shoot': 4,\n",
       " 'wedding': 4,\n",
       " 'worry': 4,\n",
       " 'calls': 4,\n",
       " 'system': 4,\n",
       " 'roommate': 4,\n",
       " 'hole': 4,\n",
       " 'middle': 4,\n",
       " 'bob': 4,\n",
       " 'essence': 4,\n",
       " 'deer': 4,\n",
       " 'sees': 4,\n",
       " 'past': 4,\n",
       " 'games': 4,\n",
       " 'train': 4,\n",
       " 'walking': 4,\n",
       " 'matt': 4,\n",
       " 'shrink': 4,\n",
       " 'check': 4,\n",
       " 'pocket': 4,\n",
       " 'hundred': 4,\n",
       " 'pounds': 4,\n",
       " 'eleven': 4,\n",
       " 'al': 4,\n",
       " 'tour': 4,\n",
       " 'coke': 4,\n",
       " 'shot': 4,\n",
       " 'leg': 4,\n",
       " 'fast': 4,\n",
       " 'support': 4,\n",
       " 'marc': 4,\n",
       " 'hometown': 4,\n",
       " 'shock': 4,\n",
       " 'keep': 4,\n",
       " 'kevin': 4,\n",
       " 'festival': 4,\n",
       " 'friday': 4,\n",
       " 'mr': 4,\n",
       " 'tears': 4,\n",
       " 'horizon': 4,\n",
       " 'luka': 4,\n",
       " 'gold': 4,\n",
       " 'jobs': 4,\n",
       " 'duty': 4,\n",
       " 'bruce': 4,\n",
       " 'meet': 4,\n",
       " 'au': 4,\n",
       " 'thoreau': 4,\n",
       " 'sloth': 4,\n",
       " 'effort': 4,\n",
       " 'madison': 4,\n",
       " 'none': 4,\n",
       " 'bottom': 4,\n",
       " 'david': 4,\n",
       " 'orange': 4,\n",
       " 'meal': 4,\n",
       " 'silence': 4,\n",
       " 'greeting': 4,\n",
       " 'thanksgiving': 4,\n",
       " 'babies': 4,\n",
       " 'betty': 4,\n",
       " 'astronauts': 4,\n",
       " 'temple': 4,\n",
       " 'sky': 4,\n",
       " 'vijay': 4,\n",
       " 'kennedys': 4,\n",
       " 'uncle': 4,\n",
       " 'bike': 4,\n",
       " 'magic': 3,\n",
       " 'pay': 3,\n",
       " 'stores': 3,\n",
       " 'singing': 3,\n",
       " 'choice': 3,\n",
       " 'asshole': 3,\n",
       " 'appointment': 3,\n",
       " 'artist': 3,\n",
       " 'so': 3,\n",
       " 'affair': 3,\n",
       " 'jesus': 3,\n",
       " 'bullet': 3,\n",
       " 'mary': 3,\n",
       " 'fun': 3,\n",
       " 'samples': 3,\n",
       " 'shopping': 3,\n",
       " 'excuse': 3,\n",
       " 'band': 3,\n",
       " 'leadership': 3,\n",
       " 'worlds': 3,\n",
       " 'marriage': 3,\n",
       " 'brooklyn': 3,\n",
       " 'park': 3,\n",
       " 'freedom': 3,\n",
       " 'temper': 3,\n",
       " 'play': 3,\n",
       " 'smoke': 3,\n",
       " 'dog': 3,\n",
       " 'brand': 3,\n",
       " 'share': 3,\n",
       " 'smoking': 3,\n",
       " 'toilet': 3,\n",
       " 'bye': 3,\n",
       " 'fair': 3,\n",
       " 'reasons': 3,\n",
       " 'city': 3,\n",
       " 'scene': 3,\n",
       " 'saw': 3,\n",
       " 'fiction': 3,\n",
       " 'truth': 3,\n",
       " 'spring': 3,\n",
       " 'afternoon': 3,\n",
       " 'beginning': 3,\n",
       " 'sure': 3,\n",
       " 'teachers': 3,\n",
       " 'talk': 3,\n",
       " 'hate': 3,\n",
       " 'animal': 3,\n",
       " 'childhood': 3,\n",
       " 'well': 3,\n",
       " 'teach': 3,\n",
       " 'cabin': 3,\n",
       " 'ba': 3,\n",
       " 'georgia': 3,\n",
       " 'told': 3,\n",
       " 'painting': 3,\n",
       " 'moments': 3,\n",
       " 'message': 3,\n",
       " 'lie': 3,\n",
       " 'independence': 3,\n",
       " 'act': 3,\n",
       " 'surprise': 3,\n",
       " 'pile': 3,\n",
       " 'families': 3,\n",
       " 'holidays': 3,\n",
       " 'therapy': 3,\n",
       " 'counselor': 3,\n",
       " 'classes': 3,\n",
       " 'instructors': 3,\n",
       " 'students': 3,\n",
       " 'chest': 3,\n",
       " 'feels': 3,\n",
       " 'drawings': 3,\n",
       " 'sex': 3,\n",
       " 'student': 3,\n",
       " 'seconds': 3,\n",
       " 'rules': 3,\n",
       " 'places': 3,\n",
       " 'list': 3,\n",
       " 'security': 3,\n",
       " 'shorts': 3,\n",
       " 'knees': 3,\n",
       " 'opening': 3,\n",
       " 'news': 3,\n",
       " 'training': 3,\n",
       " 'type': 3,\n",
       " 'calm': 3,\n",
       " 'sixty': 3,\n",
       " 'change': 3,\n",
       " 'crowd': 3,\n",
       " 'shoulders': 3,\n",
       " 'b': 3,\n",
       " 'sleep': 3,\n",
       " 'outfit': 3,\n",
       " 'sh': 3,\n",
       " 'panties': 3,\n",
       " 'self': 3,\n",
       " 'weekend': 3,\n",
       " 'pound': 3,\n",
       " 'england': 3,\n",
       " 'jerry': 3,\n",
       " 'agency': 3,\n",
       " 'weber': 3,\n",
       " 'cornelia': 3,\n",
       " 'birth': 3,\n",
       " 'thirteen': 3,\n",
       " 'culture': 3,\n",
       " 'rooms': 3,\n",
       " 'background': 3,\n",
       " 'senegal': 3,\n",
       " 'elene': 3,\n",
       " 'cousin': 3,\n",
       " 'cab': 3,\n",
       " 'drumming': 3,\n",
       " 'louder': 3,\n",
       " 'bits': 3,\n",
       " 'ink': 3,\n",
       " 'edge': 3,\n",
       " 'doll': 3,\n",
       " 'insurance': 3,\n",
       " 'machine': 3,\n",
       " 'theme': 3,\n",
       " 'photos': 3,\n",
       " 'worker': 3,\n",
       " 'member': 3,\n",
       " 'barack': 3,\n",
       " 'promise': 3,\n",
       " 'emancipation': 3,\n",
       " 'proclamation': 3,\n",
       " 'island': 3,\n",
       " 'lap': 3,\n",
       " 'exercise': 3,\n",
       " 'gulf': 3,\n",
       " 'petty': 3,\n",
       " 'officer': 3,\n",
       " 'chute': 3,\n",
       " 'plate': 3,\n",
       " 'pray': 3,\n",
       " 'chicken': 3,\n",
       " 'fights': 3,\n",
       " 'eyeball': 3,\n",
       " 'pilot': 3,\n",
       " 'quantum': 2,\n",
       " 'application': 2,\n",
       " 'customers': 2,\n",
       " 'beans': 2,\n",
       " 'term': 2,\n",
       " 'asks': 2,\n",
       " 'highway': 2,\n",
       " 'shore': 2,\n",
       " 'lake': 2,\n",
       " 'wear': 2,\n",
       " 'land': 2,\n",
       " 'vision': 2,\n",
       " 'behavior': 2,\n",
       " 'professor': 2,\n",
       " 'center': 2,\n",
       " 'stand': 2,\n",
       " 'moving': 2,\n",
       " 'hallways': 2,\n",
       " 'players': 2,\n",
       " 'sales': 2,\n",
       " 'rep': 2,\n",
       " 'religion': 2,\n",
       " 'beauty': 2,\n",
       " 'army': 2,\n",
       " 'tape': 2,\n",
       " 'date': 2,\n",
       " 'perfect': 2,\n",
       " 'cart': 2,\n",
       " 'mention': 2,\n",
       " 'appointments': 2,\n",
       " 'pastor': 2,\n",
       " 'campaign': 2,\n",
       " 'broken': 2,\n",
       " 'lipstick': 2,\n",
       " 'relatives': 2,\n",
       " 'gotten': 2,\n",
       " 'identity': 2,\n",
       " 'ability': 2,\n",
       " 'distance': 2,\n",
       " 'breath': 2,\n",
       " 'crying': 2,\n",
       " 'checking': 2,\n",
       " 'matches': 2,\n",
       " 'wind': 2,\n",
       " 'mm': 2,\n",
       " 'loving': 2,\n",
       " 'wha': 2,\n",
       " 'florida': 2,\n",
       " 'animals': 2,\n",
       " 'vet': 2,\n",
       " 'opens': 2,\n",
       " 'television': 2,\n",
       " 'crack': 2,\n",
       " 'hard': 2,\n",
       " 'reaches': 2,\n",
       " 'bedroom': 2,\n",
       " 'kitchen': 2,\n",
       " 'novel': 2,\n",
       " 'avatar': 2,\n",
       " 'ellen': 2,\n",
       " 'mark': 2,\n",
       " 'relief': 2,\n",
       " 'essay': 2,\n",
       " 'writing': 2,\n",
       " 'mask': 2,\n",
       " 'threats': 2,\n",
       " 'halloween': 2,\n",
       " 'society': 2,\n",
       " 'like': 2,\n",
       " 'gift': 2,\n",
       " 'science': 2,\n",
       " 'program': 2,\n",
       " 'deja': 2,\n",
       " 'vu': 2,\n",
       " 'stomach': 2,\n",
       " 'america': 2,\n",
       " 'situation': 2,\n",
       " 'marquis': 2,\n",
       " 'baseball': 2,\n",
       " 'motorcycle': 2,\n",
       " 'super': 2,\n",
       " 'cowboys': 2,\n",
       " 'belt': 2,\n",
       " 'n': 2,\n",
       " 'catch': 2,\n",
       " 'vermont': 2,\n",
       " 'jeans': 2,\n",
       " 'connery': 2,\n",
       " 'vague': 2,\n",
       " 'snow': 2,\n",
       " 'kill': 2,\n",
       " 'lingerie': 2,\n",
       " 'fantasy': 2,\n",
       " 'page': 2,\n",
       " 'turns': 2,\n",
       " 'block': 2,\n",
       " 'touch': 2,\n",
       " 'scream': 2,\n",
       " 'laughs': 2,\n",
       " 'patience': 2,\n",
       " 'furniture': 2,\n",
       " 'grandfather': 2,\n",
       " 'ear': 2,\n",
       " 'lobby': 2,\n",
       " 'note': 2,\n",
       " 'museum': 2,\n",
       " 'boston': 2,\n",
       " 'executives': 2,\n",
       " 'grades': 2,\n",
       " 'judy': 2,\n",
       " 'figures': 2,\n",
       " 'failure': 2,\n",
       " 'level': 2,\n",
       " 'shows': 2,\n",
       " 'feel': 2,\n",
       " 'tricia': 2,\n",
       " 'hitting': 2,\n",
       " 'dear': 2,\n",
       " 'review': 2,\n",
       " 'jeep': 2,\n",
       " 'artwork': 2,\n",
       " 'options': 2,\n",
       " 'harder': 2,\n",
       " 'fall': 2,\n",
       " 'bleachers': 2,\n",
       " 'playing': 2,\n",
       " 'fielder': 2,\n",
       " 'ball': 2,\n",
       " 'hopping': 2,\n",
       " 'ya': 2,\n",
       " 'sixteen': 2,\n",
       " 'position': 2,\n",
       " 'secretary': 2,\n",
       " 'hello': 2,\n",
       " 'put': 2,\n",
       " 'nick': 2,\n",
       " 'subject': 2,\n",
       " 'passing': 2,\n",
       " 'tie': 2,\n",
       " 'seats': 2,\n",
       " 'very': 2,\n",
       " 'altitude': 2,\n",
       " 'size': 2,\n",
       " 'heels': 2,\n",
       " 'tartabull': 2,\n",
       " 'press': 2,\n",
       " 'sox': 2,\n",
       " 'river': 2,\n",
       " 'knowing': 2,\n",
       " 'pink': 2,\n",
       " 'run': 2,\n",
       " 'mirror': 2,\n",
       " 'film': 2,\n",
       " 'steve': 2,\n",
       " 'bitch': 2,\n",
       " 'pole': 2,\n",
       " 'history': 2,\n",
       " 'monday': 2,\n",
       " \"o'clock\": 2,\n",
       " 'dressing': 2,\n",
       " 'grabs': 2,\n",
       " 'onstage': 2,\n",
       " 'leaves': 2,\n",
       " 'strange': 2,\n",
       " 'goddamn': 2,\n",
       " 'nice': 2,\n",
       " 'dress': 2,\n",
       " 'hit': 2,\n",
       " 'credit': 2,\n",
       " 'candle': 2,\n",
       " 'honor': 2,\n",
       " 'cell': 2,\n",
       " 'drive': 2,\n",
       " 'female': 2,\n",
       " 'focus': 2,\n",
       " 'frank': 2,\n",
       " 'circumstances': 2,\n",
       " 'girlfriend': 2,\n",
       " 'cans': 2,\n",
       " 'random': 2,\n",
       " 'male': 2,\n",
       " 'peace': 2,\n",
       " 'scotland': 2,\n",
       " 'trip': 2,\n",
       " 'vega': 2,\n",
       " 'policemen': 2,\n",
       " 'perform': 2,\n",
       " 'proof': 2,\n",
       " 'joe': 2,\n",
       " 'accomplice': 2,\n",
       " 'microphone': 2,\n",
       " 'weapon': 2,\n",
       " 'sing': 2,\n",
       " 'section': 2,\n",
       " 'therapist': 2,\n",
       " 'hobbleman': 2,\n",
       " 'modeling': 2,\n",
       " 'models': 2,\n",
       " 'photo': 2,\n",
       " 'bar': 2,\n",
       " 'lying': 2,\n",
       " 'inch': 2,\n",
       " 'hampshire': 2,\n",
       " 'aspirin': 2,\n",
       " 'flies': 2,\n",
       " 'couches': 2,\n",
       " 'consumer': 2,\n",
       " 'entertainment': 2,\n",
       " 'windows': 2,\n",
       " 'wonderful': 2,\n",
       " 'race': 2,\n",
       " 'dvds': 2,\n",
       " 'shower': 2,\n",
       " 'condition': 2,\n",
       " 'adult': 2,\n",
       " 'laying': 2,\n",
       " 'hearing': 2,\n",
       " 'lack': 2,\n",
       " 'acedia': 2,\n",
       " 'kinds': 2,\n",
       " 'medication': 2,\n",
       " 'millet': 2,\n",
       " 'cockerels': 2,\n",
       " 'taxi': 2,\n",
       " 'happen': 2,\n",
       " 'meanwhile': 2,\n",
       " 'pass': 2,\n",
       " 'break': 2,\n",
       " 'holes': 2,\n",
       " 'health': 2,\n",
       " 'notice': 2,\n",
       " 'till': 2,\n",
       " 'babe': 2,\n",
       " 'stuck': 2,\n",
       " 'oranges': 2,\n",
       " 'checkout': 2,\n",
       " 'lines': 2,\n",
       " 'fucking': 2,\n",
       " 'mood': 2,\n",
       " 'angle': 2,\n",
       " 'driveway': 2,\n",
       " 'dude': 2,\n",
       " 'photocopying': 2,\n",
       " 'workplace': 2,\n",
       " 'round': 2,\n",
       " 'evening': 2,\n",
       " 'flags': 2,\n",
       " 'decorations': 2,\n",
       " 'chairs': 2,\n",
       " 'tubes': 2,\n",
       " 'knee': 2,\n",
       " 'pennies': 2,\n",
       " 'memories': 2,\n",
       " 'exact': 2,\n",
       " 'public': 2,\n",
       " 'grandmother': 2,\n",
       " 'address': 2,\n",
       " 'edits': 2,\n",
       " 'earth': 2,\n",
       " 'swimmer': 2,\n",
       " 'raise': 2,\n",
       " 'gear': 2,\n",
       " 'navy': 2,\n",
       " 'parachute': 2,\n",
       " 'platform': 2,\n",
       " 'canopy': 2,\n",
       " 'sea': 2,\n",
       " 'lpu': 2,\n",
       " 'ocean': 2,\n",
       " 'wilson': 2,\n",
       " 'sunday': 2,\n",
       " 'neck': 2,\n",
       " 'stepfather': 2,\n",
       " 'surgery': 2,\n",
       " 'newton': 2,\n",
       " 'tragedy': 2,\n",
       " 'jack': 2,\n",
       " 'vacation': 2,\n",
       " 'sand': 2,\n",
       " 'ilene': 2,\n",
       " 'rotor': 2,\n",
       " 'crash': 2,\n",
       " 'truck': 2,\n",
       " 'physics': 1,\n",
       " 'happens': 1,\n",
       " 'tails': 1,\n",
       " 'summers': 1,\n",
       " 'custom': 1,\n",
       " 'database': 1,\n",
       " 'engineer': 1,\n",
       " 'find': 1,\n",
       " 'knows': 1,\n",
       " 'adults': 1,\n",
       " 'bird': 1,\n",
       " 'hippies': 1,\n",
       " 'hemp': 1,\n",
       " 'cloth': 1,\n",
       " 'hill': 1,\n",
       " 'sensation': 1,\n",
       " 'woulda': 1,\n",
       " 'biology': 1,\n",
       " 'choices': 1,\n",
       " ...}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all_words_dict sort by values\n",
    "all_words_dict = dict(sorted(all_words_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "all_words_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "854"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load grid transcript\n",
    "with open(\"data/moth_phon_words.pkl\", \"rb\") as fp:\n",
    "    _ = pickle.load(fp)\n",
    "    grid_transcript_words = pickle.load(fp)\n",
    "\n",
    "# loading textgrids\n",
    "words = dict()\n",
    "word_onset = dict()\n",
    "removed_time = 0\n",
    "DEFAULT_BAD_WORDS = [\"sentence_start\", \"sentence_end\", \"{SL}\", \"{{BR}\", \"{BR}\", \"(BR}\", \"{BR\", \"{LG}\", \"{ls}\", \"{LS}\", \"{IG}\", \"{CG}\", \"{LS)\", \"{NS}\", \"{NS_AP}\", \"{SP}\", \"sp\", \"\", \" \"]\n",
    "special_words = [\"time\", \"day\", \"people\", \"home\", \"year\", \"years\", \"life\", \"house\", \"job\", \"school\", \"room\", \"phone\", \"door\", \"hand\"]\n",
    "cnt = 0\n",
    "\n",
    "for this_story in test_stories:\n",
    "    this_story_unique = story_to_uniquestory[this_story]\n",
    "    transcript_words = grid_transcript_words[this_story_unique]\n",
    "    # correct delay\n",
    "    time_features = [(float(tp[0]), float(tp[1]), tp[2]) for tp in transcript_words]\n",
    "    time_features_corrected = get_stretched_features(time_features, MEG_SUBJECT, None, None, use_mean_rate=True)\n",
    "    # remove bad words!\n",
    "    words_this_story, word_onset_this_story = [], []\n",
    "    for t in time_features_corrected:\n",
    "        if t[2].lower() in special_words:\n",
    "            words_this_story.append(t[2])\n",
    "            word_onset_this_story.append(float(t[0]) - removed_time)\n",
    "            cnt += 1\n",
    "    words[this_story] = words_this_story\n",
    "    word_onset[this_story] = np.array(word_onset_this_story)\n",
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first five words of alternateithicatom: ['TIME', 'TIME', 'JOB', 'YEARS', 'JOB', 'JOB', 'JOB', 'JOB', 'JOB', 'PEOPLE', 'TIME', 'LIFE', 'LIFE', 'LIFE', 'TIME']\n",
      "onset times: [ 15.54472169  40.09085792  72.04375831  72.89224207  75.75712255\n",
      "  80.78813217  87.31646608 117.19307679 159.64721155 246.97116425\n",
      " 254.7772149  311.42598394 343.66836703 354.7186203  398.72997232]\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "print(f\"first five words of alternateithicatom:\", words[\"alternateithicatom1\"][:15])\n",
    "print(\"onset times:\", word_onset[\"alternateithicatom1\"][:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define lags\n",
    "sfreq = 50\n",
    "step = int(1000 / sfreq)  # 20ms\n",
    "lag_sample_start = -10\n",
    "lag_sample_end = 50\n",
    "lag_samples = np.arange(lag_sample_start, lag_sample_end + 1)\n",
    "lags = lag_samples * step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61, 8196)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get neuron response for each word onset\n",
    "neurons_onset_story, neurons_power_onset_story = [], []\n",
    "for story in test_stories:\n",
    "    word_onset_story = word_onset[story]\n",
    "    neurons_story = neurons_dict[story]\n",
    "    neurons_power_story = neurons_power_dict[story]\n",
    "    for t in word_onset_story:\n",
    "        t_sample = int(t * 50)\n",
    "        t_sample_start = t_sample + lag_sample_start\n",
    "        t_sample_end = t_sample + lag_sample_end + 1\n",
    "        if t_sample_start > 0 and t_sample_end < neurons_story.shape[0]:\n",
    "            neurons_onset_story.append(neurons_story[t_sample_start:t_sample_end, :])\n",
    "            neurons_power_onset_story.append(neurons_power_story[t_sample_start:t_sample_end, :])\n",
    "            cnt += 1\n",
    "neurons_onset_story = np.mean(neurons_onset_story, axis=0)\n",
    "neurons_power_onset_story = np.mean(neurons_power_onset_story, axis=0)\n",
    "neurons_onset_story.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meg_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
